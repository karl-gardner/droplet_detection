{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "yolov3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaT7tVw09pFBdEMBULqKSJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karl-gardner/droplet_detection/blob/master/yolov3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbirS3YgC_dg"
      },
      "source": [
        "<img src=\"https://user-images.githubusercontent.com/26833433/99805971-90f66b80-2b3d-11eb-80eb-8b45a15cb68e.jpg\">\n",
        "\n",
        "This is the **official YOLOv3 ðŸš€ notebook** authored by **Ultralytics**, and is freely available for redistribution under the [GPL-3.0 license](https://choosealicense.com/licenses/gpl-3.0/). \n",
        "For more information please visit https://github.com/ultralytics/yolov3 and https://www.ultralytics.com. Thank you!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpbVhgNeDDkQ"
      },
      "source": [
        "# 0. Setup\n",
        "\n",
        "Clone repo, install dependencies and check PyTorch and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CsvA2iGDDAG5",
        "outputId": "db377d49-d736-488f-f7db-83099b3de230"
      },
      "source": [
        "!git clone https://github.com/karl-gardner/droplet_detection  # clone repo\n",
        "%cd /content/droplet_detection/yolov3\n",
        "%pip install -qr requirements.txt  # install dependencies\n",
        "\n",
        "import torch\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "from utils.general import xywhn2xyxy\n",
        "import random\n",
        "from google.colab import files\n",
        "import csv\n",
        "\n",
        "clear_output()\n",
        "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.10.0+cu111 (Tesla P100-PCIE-16GB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Lw3sJxXlJ73u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7436d54-a709-479c-867d-a9b510de2211"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1wfzT28BOTY"
      },
      "source": [
        "# 1.1 Curl droplet dataset from roboflow (PC3DropletDetection2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data With Augmentation for Training (final_dataset)"
      ],
      "metadata": {
        "id": "A9EADxkkrlZv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3uAPACwmBOcm"
      },
      "source": [
        "%cd /content/droplet_detection\n",
        "!curl -L \"[ROBOFLOW-API-KEY]\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "clear_output()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data with No Augmentation (No_Augmentation)"
      ],
      "metadata": {
        "id": "-H3SDWiPrqo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection\n",
        "# !curl -L \"[ROBOFLOW-API-KEY]\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "# clear_output()"
      ],
      "metadata": {
        "id": "8mv-LJfSrvQ9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRC1AAs0BT97"
      },
      "source": [
        "# 1.2 Curl cell model dataset from roboflow (Cropped_Drops2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data With Augmentation for Training (final_dataset)"
      ],
      "metadata": {
        "id": "JMeQTNmRKkB_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCFocYgtBUKA"
      },
      "source": [
        "# %cd /content/droplet_detection\n",
        "# !curl -L \"[ROBOFLOW-API-KEY]\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "# clear_output()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data with No Augmentation (No_Augmentation)"
      ],
      "metadata": {
        "id": "YNL_kPdbKe-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection\n",
        "# !curl -L \"[ROBOFLOW-API-KEY]\" > roboflow.zip; unzip roboflow.zip; rm roboflow.zip\n",
        "# clear_output()"
      ],
      "metadata": {
        "id": "jmvo43UTKfPR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fJ9dhHtw1sM"
      },
      "source": [
        "# 2. Get information for annotations: Table ann\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For droplet model"
      ],
      "metadata": {
        "id": "FjBbceZHkik-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvH_QsQWwzPk"
      },
      "source": [
        "# %cd /content/droplet_detection\n",
        "\n",
        "# # Test images and labels\n",
        "# test_images = 0\n",
        "# test_classes = []\n",
        "# for f in os.listdir('test/labels'):\n",
        "#   test_images += 1\n",
        "#   with open('test/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:     \n",
        "#         line = line.split()\n",
        "#         test_classes.append(int(line[0]))\n",
        "\n",
        "# test_zero_cell = test_classes.count(0)\n",
        "# test_one_cell = test_classes.count(1)\n",
        "# test_two_cell = test_classes.count(2)\n",
        "# test_three_cell = test_classes.count(3)\n",
        "\n",
        "# print(\"Test zero cells: \", test_zero_cell)\n",
        "# print(\"Test one cells: \", test_one_cell)\n",
        "# print(\"Test two cells: \", test_two_cell)\n",
        "# print(\"Test three cells: \", test_three_cell)\n",
        "\n",
        "# combined_test_annotations = test_zero_cell + test_one_cell + test_two_cell + test_three_cell\n",
        "# print(\"combined test annotations: \",combined_test_annotations)\n",
        "\n",
        "# # Valid images and labels\n",
        "# valid_images = 0\n",
        "# valid_classes = []\n",
        "# for f in os.listdir('valid/labels'):\n",
        "#   valid_images += 1\n",
        "#   with open('valid/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:     \n",
        "#         line = line.split()\n",
        "#         valid_classes.append(int(line[0]))\n",
        "\n",
        "# valid_zero_cell = valid_classes.count(0)\n",
        "# valid_one_cell = valid_classes.count(1)\n",
        "# valid_two_cell = valid_classes.count(2)\n",
        "# valid_three_cell = valid_classes.count(3)\n",
        "\n",
        "# print(\"Valid zero cells: \", valid_zero_cell)\n",
        "# print(\"Valid one cells: \", valid_one_cell)\n",
        "# print(\"Valid two cells: \", valid_two_cell)\n",
        "# print(\"Valid three cells: \", valid_three_cell)\n",
        "\n",
        "# combined_valid_annotations = valid_zero_cell + valid_one_cell + valid_two_cell + valid_three_cell\n",
        "# print(\"combined validation annotations: \",combined_valid_annotations)\n",
        "\n",
        "# # Train images and labels\n",
        "# train_images = 0\n",
        "# train_classes = []\n",
        "# for f in os.listdir('train/labels'):\n",
        "#   train_images += 1\n",
        "#   with open('train/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         train_classes.append(int(line[0]))\n",
        "# train_images = train_images\n",
        "\n",
        "# train_zero_cell = train_classes.count(0)\n",
        "# train_one_cell = train_classes.count(1)\n",
        "# train_two_cell = train_classes.count(2)\n",
        "# train_three_cell = train_classes.count(3)\n",
        "\n",
        "# print(\"Train zero cells: \", train_zero_cell)\n",
        "# print(\"Train one cells: \", train_one_cell)\n",
        "# print(\"Train two cells: \", train_two_cell)\n",
        "# print(\"Train three cells: \", train_three_cell)\n",
        "\n",
        "# combined_train_annotations = train_zero_cell + train_one_cell + train_two_cell + train_three_cell\n",
        "# print(\"combined train annotations: \",combined_train_annotations)\n",
        "# combined_annotations = combined_test_annotations + combined_valid_annotations + combined_train_annotations\n",
        "# print(\"combined annotations: \",combined_annotations)\n",
        "\n",
        "# print(\"Total 0 Drops: \",train_classes.count(0) + test_classes.count(0)+ valid_classes.count(0))\n",
        "# print(\"Total 1 Drops: \",train_classes.count(1) + test_classes.count(1)+ valid_classes.count(1))\n",
        "# print(\"Total 2 Drops: \",train_classes.count(2) + test_classes.count(2)+ valid_classes.count(2))\n",
        "# print(\"Total 3 Drops: \",train_classes.count(3) + test_classes.count(3)+ valid_classes.count(3))\n",
        "\n",
        "# print(\"Test images: \",test_images)\n",
        "# print(\"Valid images: \",valid_images)\n",
        "# print(\"Train images: \",train_images)\n",
        "# print(\"Total number of Images: \",test_images + valid_images + train_images)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For cell model"
      ],
      "metadata": {
        "id": "6wQE08Ttkl6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection\n",
        "\n",
        "# # Test images and labels\n",
        "# test_images = 0\n",
        "# test_classes = []\n",
        "# for f in os.listdir('test/labels'):\n",
        "#   test_images += 1\n",
        "#   with open('test/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:     \n",
        "#         line = line.split()\n",
        "#         test_classes.append(int(line[0]))\n",
        "\n",
        "# test_cells = test_classes.count(0)\n",
        "# print(\"Cells in test set: \", test_cells)\n",
        "\n",
        "# # Valid images and labels\n",
        "# valid_images = 0\n",
        "# valid_classes = []\n",
        "# for f in os.listdir('valid/labels'):\n",
        "#   valid_images += 1\n",
        "#   with open('valid/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:     \n",
        "#         line = line.split()\n",
        "#         valid_classes.append(int(line[0]))\n",
        "\n",
        "# valid_cells = valid_classes.count(0)\n",
        "# print(\"Cells in validation set: \", valid_cells)\n",
        "\n",
        "\n",
        "# # Train images and labels\n",
        "# train_images = 0\n",
        "# train_classes = []\n",
        "# for f in os.listdir('train/labels'):\n",
        "#   train_images += 1\n",
        "#   with open('train/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         train_classes.append(int(line[0]))\n",
        "# train_images = train_images\n",
        "\n",
        "# train_cells = train_classes.count(0)\n",
        "# print(\"Cells in training set: \", train_cells)\n",
        "\n",
        "# combined_annotations = test_cells + valid_cells + train_cells\n",
        "# print(\"combined annotations: \",combined_annotations)\n",
        "\n",
        "# print(\"\")\n",
        "# print(\"Test images: \",test_images)\n",
        "# print(\"Valid images: \",valid_images)\n",
        "# print(\"Train images: \",train_images)\n",
        "# print(\"Total number of Images: \",test_images + valid_images + train_images)"
      ],
      "metadata": {
        "id": "_RcPZK5ckoZ8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Save Images for Annotation Examples: Figure ann"
      ],
      "metadata": {
        "id": "0ZK7u2FsdW_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def box_label(box, thick, label='', color=(128, 128, 128), txt_color=(0, 0, 0)):\n",
        "#   p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "#   cv2.rectangle(im, p1, p2, color, thickness=thick, lineType=cv2.LINE_AA)\n",
        "#   if label:\n",
        "#       tf = max(1 - 1, 1)  # font thickness\n",
        "#       w, h = cv2.getTextSize(label, 0, fontScale=1 / 3, thickness=tf)[0]  # text width, height\n",
        "#       outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
        "#       p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "#       cv2.rectangle(im, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
        "#       cv2.putText(im, label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2), 0, 1 / 3, txt_color,\n",
        "#                   thickness=tf, lineType=cv2.LINE_AA)"
      ],
      "metadata": {
        "id": "xGsea6TcdXLX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -r /annotation_examples\n",
        "# !mkdir /annotation_examples\n",
        "# %cd /content/drive/MyDrive/droplet_classification/data_files\n",
        "\n",
        "\n",
        "# for f in os.listdir(\"annotation_examples/droplet_model\"):\n",
        "#   if f.endswith(\".jpg\"):\n",
        "#     boxes = []\n",
        "#     classes = []\n",
        "#     with open(\"annotation_examples/droplet_model/\"+str(f[:-4]+\".txt\")) as labels:\n",
        "#       for line in labels.readlines():\n",
        "#         line = line.split()\n",
        "#         boxes.append([float(line[1]),float(line[2]),float(line[3]),float(line[4])])\n",
        "#         classes.append(int(line[0]))\n",
        "#     boxes = xywhn2xyxy(np.array(boxes), w=544, h=544)\n",
        "#     im = cv2.imread(\"annotation_examples/droplet_model/\" + f)\n",
        "#     !mkdir /annotation_examples/{f[:-4]}\n",
        "#     for i in range(boxes.shape[0]):\n",
        "#       cv2.imwrite(\"/annotation_examples/\"+f[:-4]+\"/box_\"+str(i)+\".png\", im[int(boxes[i][1]):int(boxes[i][3]), int(boxes[i][0]):int(boxes[i][2])])\n",
        "#     for i in range(boxes.shape[0]):\n",
        "#       class_label = classes[i]\n",
        "#       if class_label == 0:\n",
        "#         lab = \"drop_0cell\"\n",
        "#         col = (0,0,255)\n",
        "#       elif class_label == 1:\n",
        "#         lab = \"drop_1cell\"\n",
        "#         col = (0,255,255)\n",
        "#       elif class_label == 2:\n",
        "#         lab = \"drop_2cell\"\n",
        "#         col = (255,0,127)\n",
        "#       else:\n",
        "#         lab = \"drop_3cell\"\n",
        "#         col = (255,0,255) \n",
        "#       box_label(boxes[i,:],1 ,lab,col)\n",
        "#       cv2.imwrite(\"/annotation_examples/\"+f[:-4]+\".png\",im)\n",
        "\n",
        "\n",
        "# for f in os.listdir(\"annotation_examples/cell_model\"):\n",
        "#   if f.endswith(\".jpg\"):\n",
        "#     boxes = []\n",
        "#     with open(\"annotation_examples/cell_model/\"+str(f[:-4]+\".txt\")) as labels:\n",
        "#       for line in labels.readlines():\n",
        "#         line = line.split()\n",
        "#         boxes.append([float(line[1]),float(line[2]),float(line[3]),float(line[4])])\n",
        "#     boxes = xywhn2xyxy(np.array(boxes), w=544, h=544)\n",
        "#     im = cv2.imread(\"annotation_examples/cell_model/\" + f)\n",
        "#     for i in range(boxes.shape[0]):\n",
        "#       box_label(boxes[i,:], 2,color=(0,0,255))\n",
        "#     cv2.imwrite(\"/annotation_examples/\"+f[:-4]+\".png\",im)\n",
        "\n",
        "\n",
        "# !zip -r /annotation_examples.zip /annotation_examples\n",
        "# clear_output()\n",
        "# files.download(\"/annotation_examples.zip\")"
      ],
      "metadata": {
        "id": "OtP87dTtuIiW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMyc3q4RBeV7"
      },
      "source": [
        "# 4. Save cropped droplets with one or more cells"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TB04ItWXBe6y"
      },
      "source": [
        "# # Now convert ground truth labels and boxes\n",
        "# %cd /content/droplet_detection/yolov3\n",
        "# !rm -rf /cropped_drops\n",
        "# !rm /annotate_cropped.zip\n",
        "# !mkdir /cropped_drops\n",
        "\n",
        "# # Using the un-augmented dataset save around 2000 images from training, validation, and test droplets with 1 or more cells in them\n",
        "\n",
        "# counter_tot = 0\n",
        "# # First the training set\n",
        "# counter_set = 0\n",
        "# for j, im_file in enumerate(os.listdir(\"../train/images\")):\n",
        "#   if j % 5 == 0:\n",
        "#     f_label = im_file[0:-4]+\".txt\"\n",
        "#     with open(\"../train/labels/\"+f_label) as f:\n",
        "#       lines = f.readlines()\n",
        "#       rows = len(lines)\n",
        "#       boxes = []\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         if int(line[0]) == 0:\n",
        "#           continue\n",
        "#         x = float(line[1])\n",
        "#         y = float(line[2])\n",
        "#         mean_wh = (float(line[3])+float(line[4]))/2\n",
        "#         if x + mean_wh/2 > 1:\n",
        "#           x = 1 - mean_wh/2\n",
        "#         if y + mean_wh/2 > 1:\n",
        "#           y = 1 - mean_wh/2\n",
        "#         if x-mean_wh/2 < 0:\n",
        "#           x = mean_wh/2\n",
        "#         if y-mean_wh/2 < 0:\n",
        "#           y = mean_wh/2\n",
        "#         boxes.append([x,y,mean_wh,mean_wh])\n",
        "#     boxes = xywhn2xyxy(np.array(boxes), w=544, h=544)\n",
        "#     im = cv2.imread(\"../train/images/\"+im_file)\n",
        "#     for i in range(boxes.shape[0]):\n",
        "#       # May not be square by one pixel... make square\n",
        "#       if int(boxes[i,3])-int(boxes[i,1]) < int(boxes[i,2])-int(boxes[i,0]):\n",
        "#         boxes[i,3] += 1\n",
        "#       if int(boxes[i,3])-int(boxes[i,1]) > int(boxes[i,2])-int(boxes[i,0]):\n",
        "#         boxes[i,2] += 1\n",
        "#       cropped = im[int(boxes[i,1]):int(boxes[i,3]),int(boxes[i,0]):int(boxes[i,2]),:]\n",
        "#       cv2.imwrite(\"/cropped_drops/im_\"+str(counter_tot)+\".png\",cropped)\n",
        "#       counter_tot += 1\n",
        "#       counter_set += 1\n",
        "# print(\"number of images saved from train set: \",counter_set)\n",
        "\n",
        "# # Now the validation set\n",
        "# counter_set = 0\n",
        "# for j, im_file in enumerate(os.listdir(\"../valid/images\")):\n",
        "#   if j % 5 == 0:\n",
        "#     f_label = im_file[0:-4]+\".txt\"\n",
        "#     with open(\"../valid/labels/\"+f_label) as f:\n",
        "#       lines = f.readlines()\n",
        "#       rows = len(lines)\n",
        "#       boxes = []\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         if int(line[0]) == 0:\n",
        "#           continue\n",
        "#         x = float(line[1])\n",
        "#         y = float(line[2])\n",
        "#         mean_wh = (float(line[3])+float(line[4]))/2\n",
        "#         if x + mean_wh/2 > 1:\n",
        "#           x = 1 - mean_wh/2\n",
        "#         if y + mean_wh/2 > 1:\n",
        "#           y = 1 - mean_wh/2\n",
        "#         if x-mean_wh/2 < 0:\n",
        "#           x = mean_wh/2\n",
        "#         if y-mean_wh/2 < 0:\n",
        "#           y = mean_wh/2\n",
        "#         boxes.append([x,y,mean_wh,mean_wh])\n",
        "#     boxes = xywhn2xyxy(np.array(boxes), w=544, h=544)\n",
        "#     im = cv2.imread(\"../valid/images/\"+im_file)\n",
        "#     for i in range(boxes.shape[0]):\n",
        "#       # May not be square by one pixel... make square\n",
        "#       if int(boxes[i,3])-int(boxes[i,1]) < int(boxes[i,2])-int(boxes[i,0]):\n",
        "#         boxes[i,3] += 1\n",
        "#       if int(boxes[i,3])-int(boxes[i,1]) > int(boxes[i,2])-int(boxes[i,0]):\n",
        "#         boxes[i,2] += 1\n",
        "#       cropped = im[int(boxes[i,1]):int(boxes[i,3]),int(boxes[i,0]):int(boxes[i,2]),:]\n",
        "#       cv2.imwrite(\"/cropped_drops/im_\"+str(counter_tot)+\".png\",cropped)\n",
        "#       counter_tot += 1\n",
        "#       counter_set += 1\n",
        "# print(\"number of images saved from validation set: \",counter_set)\n",
        "\n",
        "\n",
        "# # Now the test set\n",
        "# counter_tot = 0\n",
        "# counter_set = 0\n",
        "# for im_file in enumerate(os.listdir(\"../test/images\")):\n",
        "#   if j % 5 == 0:\n",
        "#     f_label = im_file[0:-4]+\".txt\"\n",
        "#     with open(\"../test/labels/\"+f_label) as f:\n",
        "#       lines = f.readlines()\n",
        "#       rows = len(lines)\n",
        "#       boxes = []\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         if int(line[0]) == 0:\n",
        "#           continue\n",
        "#         x = float(line[1])\n",
        "#         y = float(line[2])\n",
        "#         mean_wh = (float(line[3])+float(line[4]))/2\n",
        "#         if x + mean_wh/2 > 1:\n",
        "#           x = 1 - mean_wh/2\n",
        "#         if y + mean_wh/2 > 1:\n",
        "#           y = 1 - mean_wh/2\n",
        "#         if x-mean_wh/2 < 0:\n",
        "#           x = mean_wh/2\n",
        "#         if y-mean_wh/2 < 0:\n",
        "#           y = mean_wh/2\n",
        "#         boxes.append([x,y,mean_wh,mean_wh])\n",
        "#     boxes = xywhn2xyxy(np.array(boxes), w=544, h=544)\n",
        "#     im = cv2.imread(\"../test/images/\"+im_file)\n",
        "#     for i in range(boxes.shape[0]):\n",
        "#       # May not be square by one pixel... make square\n",
        "#       if int(boxes[i,3])-int(boxes[i,1]) < int(boxes[i,2])-int(boxes[i,0]):\n",
        "#         boxes[i,3] += 1\n",
        "#       if int(boxes[i,3])-int(boxes[i,1]) > int(boxes[i,2])-int(boxes[i,0]):\n",
        "#         boxes[i,2] += 1\n",
        "#       cropped = im[int(boxes[i,1]):int(boxes[i,3]),int(boxes[i,0]):int(boxes[i,2]),:]\n",
        "#       cv2.imwrite(\"/cropped_drops/im_\"+str(counter_tot)+\".png\",cropped)\n",
        "#       counter_tot += 1\n",
        "#       counter_set += 1\n",
        "# print(\"number of images saved from test set: \",counter_set)\n",
        "# print(\"Number of total images saved from train, validation, and test sets: \",counter_tot)\n",
        "\n",
        "# !zip -r /annotate_cropped.zip /cropped_drops\n",
        "# clear_output()\n",
        "# files.download(\"/annotate_cropped.zip\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqvf2AWuDVlI"
      },
      "source": [
        "# 5.1 Train\n",
        "\n",
        "Download [COCO128](https://www.kaggle.com/ultralytics/coco128), a small 128-image tutorial dataset, start tensorboard and train YOLOv3 from a pretrained checkpoint for 3 epochs (note actual training is typically much longer, around **300-1000 epochs**, depending on your dataset)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATC8z86JDZwq"
      },
      "source": [
        "Train a YOLOv3 model on [COCO128](https://www.kaggle.com/ultralytics/coco128) with `--data coco128.yaml`, starting from pretrained `--weights yolov3.pt`, or from randomly initialized `--weights '' --cfg yolov3.yaml`. Models are downloaded automatically from the [latest YOLOv3 release](https://github.com/ultralytics/yolov3/releases), and **COCO, COCO128, and VOC datasets are downloaded automatically** on first use.\n",
        "\n",
        "All training results are saved to `runs/train/` with incrementing run directories, i.e. `runs/train/exp2`, `runs/train/exp3` etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oIau9GHuDcYL"
      },
      "source": [
        "# # Train YOLOv3 on COCO128 for 3 epochs\n",
        "# %cd /content/droplet_detection/yolov3\n",
        "# !python train.py --img 544 --batch 32 --epochs 200 --data ../yaml_files/cell_model.yaml --weights '' --cfg models/yolov3.yaml --cache"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iK343yD6HULj"
      },
      "source": [
        "# %cp /content/droplet_detection/yolov3/runs/train/exp/weights/best.pt /content/drive/MyDrive/droplet_classification/data_files/yolov3_cell_weights_46_best.pt"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5.2 Generate high quality mAP plots"
      ],
      "metadata": {
        "id": "9IfWKlyATQRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# map5 = []\n",
        "# map595 = []\n",
        "# with open('/content/drive/MyDrive/droplet_classification/data_files/train_results.csv', newline='') as csvfile:\n",
        "#   spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')\n",
        "#   next(spamreader)\n",
        "#   for row in spamreader:\n",
        "#     for i, elem in enumerate(row):\n",
        "#       if i == 6:\n",
        "#         map5.append(float(elem.strip()))\n",
        "#       if i == 7:\n",
        "#         map595.append(float(elem.strip()))\n",
        "# fig, axs = plt.subplots(1,2)\n",
        "# axs[0].plot(map5, color=\"blue\", marker='.')\n",
        "# axs[0].set_ylim(0, 1)\n",
        "# axs[0].tick_params(axis='both', which='major', labelsize=13)\n",
        "# axs[0].set_xlabel(\"Epochs\", fontsize=17)\n",
        "# axs[0].set_ylabel(\"mAP @ IOU 0.5\", fontsize=17)\n",
        "# axs[1].plot(map595, color=\"blue\", marker='.')\n",
        "# axs[1].set_ylim(0, 1)\n",
        "# axs[1].tick_params(axis='both', which='major', labelsize=13)\n",
        "# axs[1].set_ylabel(\"mAP @ IOU 0.5:0.95\", fontsize=17)\n",
        "# axs[1].set_xlabel(\"Epochs\", fontsize=17)\n",
        "# fig.tight_layout(pad=2.5)\n",
        "\n",
        "# fig.savefig(\"/mAP_yolov3.png\", dpi=400)\n",
        "# files.download(\"/mAP_yolov3.png\")"
      ],
      "metadata": {
        "id": "PukCi44lSz3B"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Test"
      ],
      "metadata": {
        "id": "q5vAcsQdsLbE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc848X-UYGj2"
      },
      "source": [
        "mAP calculation for droplet model test set: Table drop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CEtgforYJHn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34b4c763-f579-497c-fcbe-e996ba2bd43c"
      },
      "source": [
        "# Run YOLOv3 on COCO test-dev2017 using --task test\n",
        "%cd /content/droplet_detection/yolov3\n",
        "!python val.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --data ../yaml_files/droplet_model.yaml --task test\n",
        "\n",
        "## Download Figures\n",
        "# files.download(\"runs/val/exp/PR_curve.png\")\n",
        "# files.download(\"runs/val/exp/legend.png\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/droplet_detection/yolov3\n",
            "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
            "\u001b[34m\u001b[1mval: \u001b[0mdata=../yaml_files/droplet_model.yaml, weights=['/content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt'], batch_size=32, imgsz=544, conf_thres=0.001, iou_thres=0.6, task=test, device=, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
            "YOLOv3 ðŸš€ 57bc61a torch 1.10.0+cu111 CUDA:0 (Tesla P100-PCIE-16GB, 16281MiB)\n",
            "\n",
            "Fusing layers... \n",
            "Model Summary: 261 layers, 61513585 parameters, 0 gradients, 154.8 GFLOPs\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mScanning '../test/labels' images and labels...70 found, 0 missing, 0 empty, 0 corrupted: 100% 70/70 [00:00<00:00, 1457.19it/s]\n",
            "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: ../test/labels.cache\n",
            "               Class     Images     Labels          P          R     mAP@.5 mAP@.5:.95: 100% 3/3 [00:03<00:00,  1.17s/it]\n",
            "                 all         70       1653      0.935      0.945      0.974      0.917\n",
            "          drop_0cell         70        422       0.96      0.976      0.992      0.932\n",
            "          drop_1cell         70        468      0.937      0.956      0.979       0.92\n",
            "          drop_2cell         70        397      0.913      0.894      0.954      0.897\n",
            "          drop_3cell         70        366      0.931      0.954       0.97       0.92\n",
            "Speed: 0.1ms pre-process, 17.3ms inference, 2.2ms NMS per image at shape (32, 3, 544, 544)\n",
            "Results saved to \u001b[1mruns/val/exp\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcT8Ft4ZDlp3"
      },
      "source": [
        "mAP calculation for cell model test set: Figure gvpcell and Table cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3kVUWAbDmSU"
      },
      "source": [
        "# # Run YOLOv3 on COCO test-dev2017 using --task test\n",
        "# %cd /content/droplet_detection/yolov3\n",
        "# !rm -rf runs/val/exp\n",
        "# !python val.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_cell_46_best.pt --img 544 --data /content/droplet_detection/yaml_files/cell_model.yaml --task test\n",
        "# files.download(\"runs/val/exp/val_batch0_labels_and_pred.jpg\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7. Plot ground truth labels vs predictions for droplet model test set: Figure gvpdrop"
      ],
      "metadata": {
        "id": "wyyTHIstpFgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# !rm -rf runs/detect/exp\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source ../test/images --line-thickness 1"
      ],
      "metadata": {
        "id": "8Eq1tpbXpFqr"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def box_label(box, label='', color=(128, 128, 128), txt_color=(0, 0, 0)):\n",
        "#   p1, p2 = (int(box[0]), int(box[1])), (int(box[2]), int(box[3]))\n",
        "#   cv2.rectangle(im, p1, p2, color, thickness=1, lineType=cv2.LINE_AA)\n",
        "#   if label:\n",
        "#       tf = max(1 - 1, 1)  # font thickness\n",
        "#       w, h = cv2.getTextSize(label, 0, fontScale=1 / 3, thickness=tf)[0]  # text width, height\n",
        "#       outside = p1[1] - h - 3 >= 0  # label fits outside box\n",
        "#       p2 = p1[0] + w, p1[1] - h - 3 if outside else p1[1] + h + 3\n",
        "#       cv2.rectangle(im, p1, p2, color, -1, cv2.LINE_AA)  # filled\n",
        "#       cv2.putText(im, label, (p1[0], p1[1] - 2 if outside else p1[1] + h + 2), 0, 1 / 3, txt_color,\n",
        "#                   thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "# !rm -r /test_results\n",
        "# !rm /test_results.zip\n",
        "# !mkdir /test_results\n",
        "# !mkdir /test_results/gt_vs_pred\n",
        "# !mkdir /test_results/inputs\n",
        "# !mkdir /test_results/preds\n",
        "\n",
        "# for count, f in enumerate(os.listdir(\"../test/images\")):\n",
        "#   im_file = \"../test/images/\"+f\n",
        "#   pred_file = \"runs/detect/exp/\"+f\n",
        "#   label_file = \"../test/labels/\"+f\n",
        "#   label_file = label_file[0:-4] + \".txt\"\n",
        "  \n",
        "#   with open(label_file) as lab:\n",
        "#     lines = lab.readlines()\n",
        "#     rows = len(lines)\n",
        "#     boxes = np.zeros((rows,4))\n",
        "#     classes = []\n",
        "#     for i, line in enumerate(lines):\n",
        "#       line = line.split()\n",
        "#       classes.append(int(line[0]))\n",
        "#       boxes[i,0] = float(line[1])\n",
        "#       boxes[i,1] = float(line[2])\n",
        "#       boxes[i,2] = float(line[3])\n",
        "#       boxes[i,3] = float(line[4])\n",
        "#   boxes = xywhn2xyxy(boxes, w=544, h=544)\n",
        "\n",
        "#   im = cv2.imread(im_file)\n",
        "#   cv2.imwrite(\"/test_results/inputs/\"+f[:-4]+\".png\", im)\n",
        "#   gt_vs_pred_im = np.zeros((im.shape[0]+6*2,im.shape[1]*2+6*3,3))\n",
        "#   for i in range(boxes.shape[0]):\n",
        "#     class_label = classes[i]\n",
        "#     if class_label == 0:\n",
        "#       lab = \"drop_0cell\"\n",
        "#       col = (0,0,255)\n",
        "#     elif class_label == 1:\n",
        "#       lab = \"drop_1cell\"\n",
        "#       col = (0,255,255)\n",
        "#     elif class_label == 2:\n",
        "#       lab = \"drop_2cell\"\n",
        "#       col = (255,0,127)\n",
        "#     else:\n",
        "#       lab = \"drop_3cell\"\n",
        "#       col = (255,0,255)\n",
        "#     b = boxes[i,:]\n",
        "#     box_label(b,lab,col)\n",
        "#   gt_vs_pred_im[6 : 544 + 6, 6 : 544 + 6, :] = im\n",
        "#   im = cv2.imread(pred_file)\n",
        "#   cv2.imwrite(\"/test_results/preds/\"+f[:-4]+\".png\", im)\n",
        "#   gt_vs_pred_im[6 : 544 + 6, 544 + 12 : 544*2 + 12, :] = im\n",
        "#   cv2.imwrite(\"/test_results/gt_vs_pred/\"+f[:-4]+\".png\",gt_vs_pred_im)\n",
        "# !zip -r /droplet_test_results.zip /test_results\n",
        "# clear_output()\n",
        "# files.download(\"/droplet_test_results.zip\")"
      ],
      "metadata": {
        "id": "MgIYC4BXpOo0"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Conduct average run times across test set to get FPS"
      ],
      "metadata": {
        "id": "NN_IzzPspTyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_cell_46_best.pt --img 544 --conf-thres 0.6 --source=../test/images"
      ],
      "metadata": {
        "id": "6VozZYbUpT9g"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qFYqpthISNe"
      },
      "source": [
        "# Computer Statistics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh6G9DN6_WEc"
      },
      "source": [
        "# !nvidia-smi -L\n",
        "# !nvidia-smi"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pYU4zzjODXBy"
      },
      "source": [
        "# !lscpu |grep 'Model name'\n",
        "\n",
        "# #no.of sockets i.e available slots for physical processors\n",
        "# !lscpu | grep 'Socket(s):'\n",
        "\n",
        "# #no.of cores each processor is having \n",
        "# !lscpu | grep 'Core(s) per socket:'\n",
        "\n",
        "# #no.of threads each core is having\n",
        "# !lscpu | grep 'Thread(s) per core'\n",
        "\n",
        "# !lscpu | grep \"L3 cache\" \n",
        "\n",
        "# #if it had turbo boost it would've shown Min and Max MHz also but it is only showing current frequency this means it always operates at shown frequency\n",
        "# !lscpu | grep \"MHz\"\n",
        "\n",
        "# #memory that we can use\n",
        "# !free -h --si | awk  '/Mem:/{print $2}'\n",
        "\n",
        "# #hard disk space that we can use\n",
        "# !df -h / | awk '{print $4}'"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXb3ufUMn1LB"
      },
      "source": [
        "# 9.1 Compare Production Set with Poisson Distribution: Figure pcomp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For second trial of images between 0-6 minutes (11/9/2021)"
      ],
      "metadata": {
        "id": "fmgaSBTqnN8D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# production_images_all = sorted(os.listdir(\"/content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-9-2021_t02_0-6\"))\n",
        "# production_images = production_images_all[9000:15400]\n",
        "\n",
        "\n",
        "# !rm -rf /content/droplet_detection/yolov3/data/production_images\n",
        "# !mkdir /content/droplet_detection/yolov3/data/production_images\n",
        "# for image in production_images:\n",
        "#   !cp /content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-9-2021_t02_0-6/{image} /content/droplet_detection/yolov3/data/production_images/{image}\n",
        "\n",
        "# !rm -rf runs/detect/exp\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source /content/droplet_detection/yolov3/data/production_images --save-txt --line-thickness 1\n",
        "\n",
        "# classes = []\n",
        "# for f in os.listdir('runs/detect/exp/labels'):\n",
        "#   with open('runs/detect/exp/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         classes.append(int(line[0]))\n",
        "# zero_ML = classes.count(0)\n",
        "# one_ML = classes.count(1)\n",
        "# two_ML = classes.count(2)\n",
        "# three_ML = classes.count(3)\n",
        "# tot_ML = zero_ML + one_ML + two_ML + three_ML"
      ],
      "metadata": {
        "id": "zcRuxhFSLGfK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For second trial of images between 87-89 minutes (11/9/2021)"
      ],
      "metadata": {
        "id": "UbAYDIJ3y46r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# production_images_all = sorted(os.listdir(\"/content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-9-2021_t02_87-89\"))\n",
        "# production_images = production_images_all[0:6400]\n",
        "\n",
        "\n",
        "# !rm -rf /content/droplet_detection/yolov3/data/production_images\n",
        "# !mkdir /content/droplet_detection/yolov3/data/production_images\n",
        "# for image in production_images:\n",
        "#   !cp /content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-9-2021_t02_87-89/{image} /content/droplet_detection/yolov3/data/production_images/{image}\n",
        "\n",
        "# !rm -rf runs/detect/exp\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source /content/droplet_detection/yolov3/data/production_images --save-txt --line-thickness 1\n",
        "\n",
        "# classes = []\n",
        "# for f in os.listdir('runs/detect/exp/labels'):\n",
        "#   with open('runs/detect/exp/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         classes.append(int(line[0]))\n",
        "# zero_ML_2 = classes.count(0)\n",
        "# one_ML_2 = classes.count(1)\n",
        "# two_ML_2 = classes.count(2)\n",
        "# three_ML_2 = classes.count(3)\n",
        "# tot_ML_2 = zero_ML_2 + one_ML_2 + two_ML_2 + three_ML_2"
      ],
      "metadata": {
        "id": "GCpCkElEy5G3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For third trial of images between 0-3 minutes (11/15/2021)"
      ],
      "metadata": {
        "id": "0x0RIWKZu1_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# production_images_all = sorted(os.listdir(\"/content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_0-3\"))\n",
        "# production_images = production_images_all[3000:9400]\n",
        "\n",
        "\n",
        "# !rm -rf /content/droplet_detection/yolov3/data/production_images\n",
        "# !mkdir /content/droplet_detection/yolov3/data/production_images\n",
        "# for image in production_images:\n",
        "#   !cp /content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_0-3/{image} /content/droplet_detection/yolov3/data/production_images/{image}\n",
        "\n",
        "# !rm -rf runs/detect/exp\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source /content/droplet_detection/yolov3/data/production_images --save-txt --line-thickness 1\n",
        "\n",
        "# classes = []\n",
        "# for f in os.listdir('runs/detect/exp/labels'):\n",
        "#   with open('runs/detect/exp/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         classes.append(int(line[0]))\n",
        "# zero_ML = classes.count(0)\n",
        "# one_ML = classes.count(1)\n",
        "# two_ML = classes.count(2)\n",
        "# three_ML = classes.count(3)\n",
        "# tot_ML = zero_ML + one_ML + two_ML + three_ML"
      ],
      "metadata": {
        "id": "dk6jFDY7u2KE"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For third trial of images between 59-60 minutes (11/15/2021)"
      ],
      "metadata": {
        "id": "me8W3meO1GXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# production_images_all = sorted(os.listdir(\"/content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_59-60\"))\n",
        "# production_images = production_images_all[0:6400]\n",
        "\n",
        "\n",
        "# !rm -rf /content/droplet_detection/yolov3/data/production_images\n",
        "# !mkdir /content/droplet_detection/yolov3/data/production_images\n",
        "# for image in production_images:\n",
        "#   !cp /content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_59-60/{image} /content/droplet_detection/yolov3/data/production_images/{image}\n",
        "\n",
        "# !rm -rf runs/detect/exp\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source /content/droplet_detection/yolov3/data/production_images --save-txt --line-thickness 1\n",
        "\n",
        "# classes = []\n",
        "# for f in os.listdir('runs/detect/exp/labels'):\n",
        "#   with open('runs/detect/exp/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         classes.append(int(line[0]))\n",
        "# zero_ML_2 = classes.count(0)\n",
        "# one_ML_2 = classes.count(1)\n",
        "# two_ML_2 = classes.count(2)\n",
        "# three_ML_2 = classes.count(3)\n",
        "# tot_ML_2 = zero_ML_2 + one_ML_2 + two_ML_2 + three_ML_2"
      ],
      "metadata": {
        "id": "WeHWkLuD1GlQ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot Poisson Distribution Curves"
      ],
      "metadata": {
        "id": "DGpPxYkpS8oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# c_0 = 7e6\n",
        "# d = 74e-6\n",
        "# v_0 = (4/3)*math.pi*(d/2)**3\n",
        "# v_0 = v_0*10**6\n",
        "\n",
        "# # Calculate lambda as the expected value or the average number of cells per nanoliter drop\n",
        "# lam = c_0*v_0\n",
        "\n",
        "# Pr = []\n",
        "# K = [0,1,2]\n",
        "# for k in K:\n",
        "#   Pr.append((lam**k)*math.exp(-1*lam)/math.factorial(k))\n",
        "# K.append(3)\n",
        "# Pr.append(1-sum(Pr))\n",
        "\n",
        "# plt.plot(K,[zero_ML/tot_ML,one_ML/tot_ML,two_ML/tot_ML,three_ML/tot_ML],linestyle='--', marker='o', color='r', label=\"YOLOv3 Model \" u\"\\u2248\" \" 0-64 seconds\")\n",
        "# # plt.plot(K,[zero_ML_2/tot_ML_2,one_ML_2/tot_ML_2,two_ML_2/tot_ML_2,three_ML_2/tot_ML_2],linestyle='--', marker='o', color='maroon', label=\"YOLOv3 Model \" u\"\\u2248\" \" 5220-5284 seconds\")\n",
        "# plt.plot(K,Pr, marker='o', color='b', label='Poisson Distribution')\n",
        "\n",
        "\n",
        "# plt.xticks(K,(\"0\",\"1\",\"2\",\">3\"))\n",
        "# plt.xlabel(\"k\")\n",
        "# # plt.ylabel(\"P(x=k)\")\n",
        "# plt.ylabel(\"% of Droplets\")\n",
        "# # plt.title(\"Trial 1: 12800 images\")\n",
        "\n",
        "# plt.legend()\n",
        "\n",
        "# plt.savefig(\"/trial_1.png\",dpi=500)\n",
        "# files.download(\"/trial_1.png\")"
      ],
      "metadata": {
        "id": "z5yWJGV6RyDm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9.2 Compare Production Set with Hand Counted Percentages: Figure hcomp"
      ],
      "metadata": {
        "id": "oXd33EIlTB3H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run 500 images from second trial and compare droplet ratios with hand counting (Trial 1)"
      ],
      "metadata": {
        "id": "Nv4JPOuSbZp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# production_images_all = sorted(os.listdir(\"/content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-9-2021_t02_0-6\"))\n",
        "# production_images = production_images_all[1000:1500]\n",
        "\n",
        "\n",
        "# !rm -rf /content/droplet_detection/yolov3/data/production_images\n",
        "# !mkdir /content/droplet_detection/yolov3/data/production_images\n",
        "# for image in production_images:\n",
        "#   !cp /content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-9-2021_t02_0-6/{image} /content/droplet_detection/yolov3/data/production_images/{image}\n",
        "\n",
        "# # !zip -r /hand_count.zip /content/droplet_detection/yolov3/data/production_images\n",
        "# # files.download(\"/hand_count.zip\")\n",
        "\n",
        "# !rm -rf runs/detect/exp\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source /content/droplet_detection/yolov3/data/production_images --save-txt --line-thickness 1\n",
        "\n",
        "# classes = []\n",
        "# for f in os.listdir('runs/detect/exp/labels'):\n",
        "#   with open('runs/detect/exp/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         classes.append(int(line[0]))\n",
        "# zero_ML = classes.count(0)\n",
        "# one_ML = classes.count(1)\n",
        "# two_ML = classes.count(2)\n",
        "# three_ML = classes.count(3)\n",
        "# tot_ML = zero_ML + one_ML + two_ML + three_ML\n",
        "\n",
        "# # For hand counting of 500 images\n",
        "# zero_BH = 1993\n",
        "# one_BH = 3399\n",
        "# two_BH = 2658\n",
        "# three_BH = 3530\n",
        "# tot_BH = zero_BH + one_BH + two_BH + three_BH"
      ],
      "metadata": {
        "id": "wqb_h6YdbZ3x"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run 500 images from third trial and compare droplet ratios with hand counting (Trial 2)"
      ],
      "metadata": {
        "id": "VIm_L2pzmJrd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd /content/droplet_detection/yolov3\n",
        "# production_images_all = sorted(os.listdir(\"/content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_0-3\"))\n",
        "# production_images = production_images_all[1000:1500]\n",
        "\n",
        "\n",
        "# !rm -rf /content/droplet_detection/yolov3/data/production_images\n",
        "# !mkdir /content/droplet_detection/yolov3/data/production_images\n",
        "# for image in production_images:\n",
        "#   !cp /content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_0-3/{image} /content/droplet_detection/yolov3/data/production_images/{image}\n",
        "\n",
        "# # !zip -r /hand_count.zip /content/droplet_detection/yolov3/data/production_images\n",
        "# # files.download(\"/hand_count.zip\")\n",
        "\n",
        "# !rm -rf runs/detect/exp\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source /content/droplet_detection/yolov3/data/production_images --save-txt --line-thickness 1\n",
        "\n",
        "# classes = []\n",
        "# for f in os.listdir('runs/detect/exp/labels'):\n",
        "#   with open('runs/detect/exp/labels/' + f) as read_file:\n",
        "#       lines = read_file.readlines()\n",
        "#       for line in lines:\n",
        "#         line = line.split()\n",
        "#         classes.append(int(line[0]))\n",
        "# zero_ML = classes.count(0)\n",
        "# one_ML = classes.count(1)\n",
        "# two_ML = classes.count(2)\n",
        "# three_ML = classes.count(3)\n",
        "# tot_ML = zero_ML + one_ML + two_ML + three_ML\n",
        "\n",
        "# # For hand counting of 500 images\n",
        "# zero_BH = 1610\n",
        "# one_BH = 3068\n",
        "# two_BH = 3827\n",
        "# three_BH = 3785\n",
        "# tot_BH = zero_BH + one_BH + two_BH + three_BH"
      ],
      "metadata": {
        "id": "31Qk7_hFmJ4J"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fRXETP7JBtR"
      },
      "source": [
        "Plot ML vs Hand Distribution Curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EfqtssDJB70"
      },
      "source": [
        "# K = [0,1,2,3]\n",
        "# plt.plot(K,[zero_ML/tot_ML,one_ML/tot_ML,two_ML/tot_ML,three_ML/tot_ML],linestyle='--', marker='o', color='r', label=\"YOLOv3 Model\")\n",
        "# plt.plot(K,[zero_BH/tot_BH,one_BH/tot_BH,two_BH/tot_BH,three_BH/tot_BH],linestyle='--', marker='o', color='g', label='Hand Counted')\n",
        "\n",
        "# plt.xticks(K,(\"0\",\"1\",\"2\",\">3\"))\n",
        "# plt.xlabel(\"k\")\n",
        "# # plt.ylabel(\"P(x=k)\")\n",
        "# plt.ylabel(\"% of Droplets\")\n",
        "# plt.title(\"Trial 2: \" u\"\\u2248\" \" 0-5 seconds (500 images)\")\n",
        "\n",
        "# plt.legend(loc='lower right')\n",
        "\n",
        "# plt.savefig(\"/trial_2.png\",dpi=500)\n",
        "# files.download(\"/trial_2.png\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.1 Conduct inference on production set with both models: Figure ework"
      ],
      "metadata": {
        "id": "bVDra5jHPQAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # remove and make appropriate directories\n",
        "# %cd /content/droplet_detection/yolov3\n",
        "# !rm -rf runs/cropped_drops\n",
        "# !rm -rf runs/detect/exp\n",
        "# !rm -rf runs/detect/exp2\n",
        "\n",
        "# prod_image = \"test_03002_Cam_V710_Cine1.png\"\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_243_best.pt --img 544 --conf-thres 0.6 --source /content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_0-3/{prod_image} --save-txt --line-thickness 2 --hide-labels --hide-conf\n",
        "# files.download(\"runs/detect/exp/\" + prod_image)\n",
        "\n",
        "# !mkdir runs/cropped_drops\n",
        "# counter = 0\n",
        "# with open(\"runs/detect/exp/labels/\"+prod_image[:-4]+\".txt\") as f:\n",
        "#   boxes = []\n",
        "#   for i, line in enumerate(f.readlines()):\n",
        "#     line = line.split()\n",
        "#     x = float(line[1])\n",
        "#     y = float(line[2])\n",
        "#     mean_wh = (float(line[3])+float(line[4]))/2\n",
        "#     if x + mean_wh/2 > 1:\n",
        "#       x = 1 - mean_wh/2\n",
        "#     if y + mean_wh/2 > 1:\n",
        "#       y = 1 - mean_wh/2\n",
        "#     if x-mean_wh/2 < 0:\n",
        "#       x = mean_wh/2\n",
        "#     if y-mean_wh/2 < 0:\n",
        "#       y = mean_wh/2\n",
        "#     boxes.append([x,y,mean_wh,mean_wh])\n",
        "#   boxes = xywhn2xyxy(np.array(boxes), w=544, h=544)\n",
        "#   im = cv2.imread(\"/content/drive/MyDrive/droplet_classification/data_files/production_set/data_11-15-2021_t03_0-3/\"+prod_image)\n",
        "#   for i in range(boxes.shape[0]):\n",
        "#     # May not be square by one pixel... make square\n",
        "#     if int(boxes[i,3])-int(boxes[i,1]) < int(boxes[i,2])-int(boxes[i,0]):\n",
        "#       boxes[i,3] += 1\n",
        "#     if int(boxes[i,3])-int(boxes[i,1]) > int(boxes[i,2])-int(boxes[i,0]):\n",
        "#       boxes[i,2] += 1\n",
        "#     cropped_resized = cv2.resize(im[int(boxes[i,1]):int(boxes[i,3]),int(boxes[i,0]):int(boxes[i,2])],(544, 544))\n",
        "#     cv2.imwrite(\"runs/cropped_drops/cropped_drop_\"+str(counter)+\".png\",cropped_resized)\n",
        "#     counter += 1\n",
        "\n",
        "# !python detect.py --weights /content/drive/MyDrive/droplet_classification/data_files/yolov3_weights_cell_46_best.pt --img 544 --conf-thres 0.6 --source runs/cropped_drops --line-thickness 2 --hide-labels --hide-conf\n",
        "# !zip -r /cropped_pred.zip /content/droplet_detection/yolov3/runs/detect/exp2\n",
        "# files.download(\"/cropped_pred.zip\")"
      ],
      "metadata": {
        "id": "t5C8dIx0PQL-"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10.2 Construct Poisson Curve for multiple lambdas: Figure ework"
      ],
      "metadata": {
        "id": "AS9yA7NPUuCY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prs = []\n",
        "# lambdas = [1,3,6,9]\n",
        "# colors = [\"r\",\"g\",\"b\",\"black\",]\n",
        "# K = list(range(0,21,1))\n",
        "\n",
        "# for lam,c in zip(lambdas,colors):\n",
        "#   Pr = []\n",
        "#   for k in K:\n",
        "#     Pr.append((lam**k)*math.exp(-1*lam)/math.factorial(k))\n",
        "  \n",
        "#   plt.plot(K,Pr, markersize=3.5,marker='o', color=c, label=\"\\u03BB = \" + str(lam))\n",
        "\n",
        "# plt.xticks([0,5,10,15,20],[\"0\",\"5\",\"10\",\"15\",\"20\"])\n",
        "# plt.yticks([0.0,0.1,0.2,0.3,0.4],[\"0.0\",\"0.1\",\"0.2\",\"0.2\",\"0.3\",\"0.4\"])\n",
        "# plt.xlabel(\"k\")\n",
        "# plt.ylabel(\"Pr(X=k)\")\n",
        "\n",
        "# plt.legend()\n",
        "\n",
        "# plt.savefig(\"/poisson_distribution.png\",dpi=500)\n",
        "# files.download(\"/poisson_distribution.png\")"
      ],
      "metadata": {
        "id": "BRN6OLG0UuNx"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}